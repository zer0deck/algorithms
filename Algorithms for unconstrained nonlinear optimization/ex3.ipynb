{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libs initialization\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tkinter as tk\n",
    "import tkinter.ttk as ttk\n",
    "from tkinter import *\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from scipy.optimize import minimize\n",
    "from tqdm.notebook import tqdm \n",
    "import math\n",
    "\n",
    "#parameters\n",
    "alpha = random.uniform (0, 1)\n",
    "beta = random.uniform (0, 1)\n",
    "\n",
    "y = []\n",
    "x = []\n",
    "a = []\n",
    "b = []\n",
    "x_lma = np.arange(0, 101, 1) / 100\n",
    "y_lma = alpha * x_lma + beta + random.normalvariate (0.5, 0.125)\n",
    "\n",
    "#functions creation\n",
    "def func (x):\n",
    "    y = alpha * x + beta + random.normalvariate (0.5, 0.125)\n",
    "    return y\n",
    "\n",
    "#Arrays filling loops\n",
    "for k in range (0, 101, 1):\n",
    "    y.append( func (k / 100 ))\n",
    "    x.append(k/100)\n",
    "\n",
    "xk = np.array(x)\n",
    "yk = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient descent\n",
    "a = 0.3\n",
    "b = 0.3\n",
    "ab_0 = [a, b]\n",
    "ab_i=[[0.25,0.25]]\n",
    "ab_i.append(ab_0)\n",
    "beta_i = []\n",
    "grad_i = [[0.1, 0.1]]\n",
    "i=1\n",
    "while abs(ab_i[i][0] - ab_i[i-1][0]) > 0.001 and abs(ab_i[i][1] - ab_i[i-1][1]) > 0.001:\n",
    "    grad = [(np.sum(2 * xk * (ab_i[i][0] * xk + ab_i[i][1] - yk)))/len(xk), (np.sum(2 * (ab_i[i][0] * xk + ab_i[i][1] - yk)))/len(xk)]\n",
    "    grad_i.append(grad)\n",
    "    beta = abs((np.array(ab_i[i])-np.array(ab_i[i-1]))*(np.array(grad_i[i])-np.array(grad_i[i-1])))/(abs(np.array(grad_i[i])-np.array(grad_i[i-1])))**2\n",
    "    beta_i.append(beta.tolist())\n",
    "    x_next = np.array(ab_i[i]) - beta_i[i-1] * np.array(grad_i[i])\n",
    "    ab_i.append(x_next.tolist())\n",
    "    i += 1\n",
    "ex_1_lin = ab_i[i]\n",
    "\n",
    "a = 6\n",
    "b = -0.3\n",
    "ab_0 = [a, b]\n",
    "ab_i=[[1,-0.5]]\n",
    "ab_i.append(ab_0)\n",
    "beta_i = []\n",
    "grad_i = [[0.1, 0.1]]\n",
    "i=1\n",
    "while abs(ab_i[i][0] - ab_i[i-1][0]) > 0.001 and abs(ab_i[i][1] - ab_i[i-1][1]) > 0.001:\n",
    "    grad = [(np.sum((ab_i[i][0]/(1+ab_i[i][1]*xk)-yk)*2/(1+ab_i[i][1]*xk)))/len(xk), (np.sum(2*ab_i[i][0]*xk/(1+ab_i[i][1]*xk)**2*(ab_i[i][0]/(1+ab_i[i][1]*xk)-yk)))/len(xk)]\n",
    "    grad_i.append(grad)\n",
    "    beta = abs((np.array(ab_i[i])-np.array(ab_i[i-1]))*(np.array(grad_i[i])-np.array(grad_i[i-1])))/(abs(np.array(grad_i[i])-np.array(grad_i[i-1])))**2\n",
    "    beta_i.append(beta.tolist())\n",
    "    x_next = np.array(ab_i[i]) - beta_i[i-1] * np.array(grad_i[i])\n",
    "    ab_i.append(x_next.tolist())\n",
    "    i += 1\n",
    "ex_1_rat = ab_i[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conjugate gradient \n",
    "def golden(func, limits, eps):\n",
    "    a, b = limits\n",
    "    flag = 0\n",
    "    iteration = 0\n",
    "    evaluation = 0\n",
    "    while abs(a - b) >= eps:\n",
    "        iteration += 1\n",
    "        delta = (3 - math.sqrt(5)) * (b - a) / 2\n",
    "        if flag == 0:\n",
    "            evaluation += 2\n",
    "            x1, x2 = a + delta, b - delta\n",
    "            f1, f2 = func( x1 ), func( x2 )\n",
    "        if flag == \"x2=x1\":\n",
    "            evaluation += 1\n",
    "            x1, x2 = a + delta, x1\n",
    "            f1, f2 = func( x1 ), f1\n",
    "        if flag == \"x1=x2\":\n",
    "            evaluation += 1\n",
    "            x1, x2 = x2, b - delta\n",
    "            f1, f2 = f2, func( x2 )\n",
    "\n",
    "        if f1 <= f2:\n",
    "            b = x2\n",
    "            flag = 'x2=x1'\n",
    "        else:\n",
    "            a = x1\n",
    "            flag = 'x1=x2'     \n",
    "    min_x = (a + b) / 2\n",
    "    min_y = (f1 + f2)  / 2\n",
    "    return min_x, min_y, iteration, evaluation\n",
    "def conjugate_gradient(loss, gradient, start, tol=1e-3, max_iter=1000):\n",
    "    vector = start\n",
    "    diff = -gradient(vector)\n",
    "    def alpha_min(alpha, func, diff, vector):\n",
    "        return func(vector + alpha * diff)\n",
    "    f_min = lambda alpha: alpha_min(alpha, loss, diff, vector)\n",
    "    alpha = golden(f_min, limits=(1e-3, 1e1), eps=1e-3)[0]\n",
    "    vector += alpha * diff\n",
    "    diff0 = diff\n",
    "    iterations = 0\n",
    "    for _ in tqdm(range(max_iter)):\n",
    "        iterations += 1\n",
    "        diff = -gradient(vector)\n",
    "        betta = diff.T * diff / diff0.T / diff0\n",
    "        diff += betta * diff0\n",
    "        if np.all( np.abs(diff) <=  tol):\n",
    "            print(\"Criterion stop, iterations = \", iterations)\n",
    "            break\n",
    "        alpha = golden(f_min, limits=(1e-3, 1e1), eps=1e-3)[0]\n",
    "        vector += alpha * diff\n",
    "        diff0 = diff\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.329315\n",
      "         Iterations: 2\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.312175\n",
      "         Iterations: 10\n",
      "         Function evaluations: 68\n",
      "         Gradient evaluations: 22\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.329955\n",
      "         Iterations: 32\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 34\n",
      "         Hessian evaluations: 32\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 2.866744\n",
      "         Iterations: 1\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 12\n",
      "         Hessian evaluations: 2\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.329319\n",
      "         Iterations: 33\n",
      "         Function evaluations: 62\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 1.312183\n",
      "         Iterations: 31\n",
      "         Function evaluations: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7f/v45zg7ms5gzd2qjc6tqr5nwh0000gn/T/ipykernel_22059/3080267974.py:78: OptimizeWarning: Unknown solver options: xtol\n",
      "  ex_2_lin = minimize(lin, [1., 1.], method='CG', options={'xtol': 1e-3, 'disp':True})\n",
      "/var/folders/7f/v45zg7ms5gzd2qjc6tqr5nwh0000gn/T/ipykernel_22059/3080267974.py:80: OptimizeWarning: Unknown solver options: xtol\n",
      "  ex_2_rat = minimize(rat, [1., -0.5], method='CG', options={'xtol': 1e-3, 'disp':True})\n"
     ]
    }
   ],
   "source": [
    "def lin(ab):\n",
    "    a, b = ab\n",
    "    s = 0\n",
    "    for k in range(0, 101):\n",
    "        s += (a * x[k] + b - y[k]) ** 2\n",
    "    return s\n",
    "def lin_jac(ab):\n",
    "    a, b = ab\n",
    "    l = 0\n",
    "    r = 0\n",
    "    s = 0\n",
    "    for k in range(0, 101):\n",
    "        l += (2 * x[k] * (b + a * x[k] - y[k]))\n",
    "        r += (2 * (b + a * x[k] - y[k]))\n",
    "    s = np.array([l, r])\n",
    "    return s\n",
    "def lin_hess(ab):\n",
    "    a, b = ab\n",
    "    hess = np.ones([2,2])\n",
    "    lh = 0\n",
    "    rh = 0\n",
    "    ld = 0\n",
    "    rd = 0\n",
    "    for k in range(0, 101):\n",
    "        lh += (2 * x[k]**2)\n",
    "        rh += (2 * x[k])\n",
    "        ld += (2 * x[k])\n",
    "        rd = (2)\n",
    "    hess[0,0] = lh\n",
    "    hess[0,1] = rh\n",
    "    hess[1,0] = ld\n",
    "    hess[1,1] = rd\n",
    "    return hess\n",
    "def lin_lma(ab):\n",
    "    a, b = ab\n",
    "    global x_lma, y_lma\n",
    "    return ( ((a * x_lma + b) - y_lma)**2)\n",
    "\n",
    "def rat(ab):\n",
    "    a, b = ab\n",
    "    s = 0\n",
    "    for k in range(0, 101):\n",
    "        s += (a / (1+x[k]*b)- y[k]) ** 2\n",
    "    return s\n",
    "def rat_jac(ab):\n",
    "    a, b = ab\n",
    "    l = 0\n",
    "    r = 0\n",
    "    s = 0\n",
    "    for k in range(0, 101):\n",
    "        l += ((a/(1+b*x[k]) - y[k])*2/(1+b*x[k]))\n",
    "        r += (2*a*x[k]/(1+b*x[k])**2 * (a/(1+b*x[k])-y[k]))\n",
    "    s = np.array([l, r])\n",
    "    return s\n",
    "def rat_hess(ab):\n",
    "    a, b = ab\n",
    "    hess = np.ones([2,2])\n",
    "    lh = 0\n",
    "    rh = 0\n",
    "    ld = 0\n",
    "    rd = 0\n",
    "    for k in range(0, 101):\n",
    "        lh += (2 / (1+b*x[k])**2)\n",
    "        rh += (-2*a*x[k]/(1+b*x[k])**3-2*x[k]*(a/(1+b*x[k])-y[k]) / (1+b*x[k])**2)\n",
    "        ld += (-2*a*x[k]/(1+b*x[k])**3-2*x[k]*(a/(1+b*x[k])-y[k])/(1+b*x[k])**3)\n",
    "        rd += (2 * a**2 * x[k]**2 / (1+b*x[k])**4 * 4 * a * x[k]**2 * (a/(1+b*x[k])-y[k]) / (1+b*x[k])**3)\n",
    "    hess[0,0] = lh\n",
    "    hess[0,1] = rh\n",
    "    hess[1,0] = ld\n",
    "    hess[1,1] = rd\n",
    "    return hess\n",
    "def rat_lma(ab):\n",
    "    a, b = ab\n",
    "    global x_lma, y_lma\n",
    "    return ( ((a / ( 1 + b * x_lma )) - y_lma)**2)\n",
    "\n",
    "\n",
    "ex_2_lin = minimize(lin, [1., 1.], method='CG', options={'xtol': 1e-3, 'disp':True})\n",
    "ex_2_lin = ex_2_lin.x\n",
    "ex_2_rat = minimize(rat, [1., -0.5], method='CG', options={'xtol': 1e-3, 'disp':True})\n",
    "ex_2_rat = ex_2_rat.x \n",
    "# ex_2_lin = conjugate_gradient(lin, lin_jac, [1., 1.], max_iter=1000)\n",
    "# ex_2_rat = conjugate_gradient(rat, rat_jac, [1, -0.1], max_iter=10000)\n",
    "\n",
    "#Newton algorithm\n",
    "ex_3_lin = minimize(lin, [1., 1.], method='Newton-CG', jac=lin_jac, hess=lin_hess, options={'xtol': 1e-3, 'disp':True})\n",
    "ex_3_rat = minimize(rat, [1., -0.5], method='Newton-CG', jac=rat_jac, hess=rat_hess, options={'xtol': 1e-3, 'disp':True})\n",
    "\n",
    "# Levenberg-Marquardt algorithm (LMA)\n",
    "ex_4_lin = optimize.least_squares(lin_lma, [1., 1.], method=\"lm\")\n",
    "ex_4_rat = optimize.least_squares(rat_lma, [1., 1.], method=\"lm\")\n",
    "\n",
    "#Nedler\n",
    "result1 = minimize(lin,[0.3, 0.3], method='nelder-mead', options={'xatol': 0.001,'disp': True})\n",
    "result2 = minimize(rat,[0.3, 0.3], method='nelder-mead', options={'xatol': 0.001,'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def table_output_linear ():\n",
    "    matplotlib.use('TkAgg')\n",
    "    top = Toplevel(gui)\n",
    "    fig = plt.figure()\n",
    "    # Special type of \"canvas\" to allow for matplotlib graphing\n",
    "    canvas = matplotlib.backends.backend_tkagg.FigureCanvasTkAgg(fig, master=top)\n",
    "    plot_widget = canvas.get_tk_widget()\n",
    "    plt.scatter(x,y)\n",
    "    plt.plot(xk, ex_1_lin[0]*xk+ex_1_lin[1], 'r', label='gd alg')\n",
    "    plt.plot(xk, ex_2_lin[0]*xk+ex_2_lin[1], 'g', label='cgd alg')\n",
    "    plt.plot(xk, ex_3_lin.x[0]*xk+ex_3_lin.x[1], 'y', label='newton alg')\n",
    "    plt.plot(xk, ex_4_lin.x[0]*xk+ex_4_lin.x[1], 'b', label='lma')\n",
    "    plt.plot(xk, result1.x[0]*xk+result1.x[1], 'y', label='nedler alg')\n",
    "    plt.ylabel('y(x) = α‎ * x(k) + β + δ(k)', fontsize=12)\n",
    "    plt.xlabel('x(k) = k / 100, k = [0; 1]', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plot_widget.grid(row=0, column=0)\n",
    "    top.mainloop()\n",
    "def table_output_rational ():\n",
    "    matplotlib.use('TkAgg')\n",
    "    top = Toplevel(gui)\n",
    "    fig = plt.figure()\n",
    "    # Special type of \"canvas\" to allow for matplotlib graphing\n",
    "    canvas = matplotlib.backends.backend_tkagg.FigureCanvasTkAgg(fig, master=top)\n",
    "    plot_widget = canvas.get_tk_widget()\n",
    "    plt.scatter(x,y)\n",
    "    plt.plot(xk, ex_1_rat[0] / (1 + ex_1_rat[1] * xk), 'r', label='gd alg')\n",
    "    plt.plot(xk, ex_2_rat[0] / (1 + ex_2_rat[1] * xk), 'g', label='cgd alg')\n",
    "    plt.plot(xk, ex_3_rat.x[0] / (1 + ex_3_rat.x[1] * xk), 'y', label='newton alg')\n",
    "    plt.plot(xk, ex_4_rat.x[0] / (1 + ex_4_rat.x[1] * xk), 'b', label='lma')\n",
    "    plt.plot(xk, result2.x[0] / (1 + result2.x[1] * xk), 'y', label='nedler alg')\n",
    "    plt.ylabel('y(x) = α‎ * x(k) + β + δ(k)', fontsize=12)\n",
    "    plt.xlabel('x(k) = k / 100, k = [0; 1]', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plot_widget.grid(row=0, column=0)\n",
    "    top.mainloop()\n",
    "\n",
    "#GUI\n",
    "gui = Tk ()\n",
    "gui.title('Alogorithms for unconstrained non-linear optimization')\n",
    "gui.geometry(\"300x300\")\n",
    "\n",
    "#buttons\n",
    "label1 = Label ( text = 'Ex.2. y(x) = α‎ * x(k) + β + δ(k), x(k) = k / 100 \\nChoose a plot below', fg='black', bg='azure', width=20, height=5)\n",
    "label1.pack(fill=tk.X)\n",
    "btn4_1 = Button (text = 'a* x(k) + b', fg='black', bg='lemon chiffon', width=20, height=5, command=table_output_linear)\n",
    "btn4_1.pack(fill=tk.X)\n",
    "btn4_2 = Button (text = 'a / (x(k) * b + 1)', fg='black', bg='lemon chiffon', width=20, height=5, command=table_output_rational)\n",
    "btn4_2.pack(fill=tk.X)\n",
    "\n",
    "gui.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7f/v45zg7ms5gzd2qjc6tqr5nwh0000gn/T/ipykernel_22059/2129337021.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_ex_4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_3\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_ex_4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mex_4_gauss_arr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0ma_3_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex_4_gauss_arr_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_4_gauss_arr_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#Old methods from Ex.2\n",
    "arr_ex_1_2_x = []\n",
    "arr_ex_3_x = []\n",
    "arr_ex_1 = []\n",
    "arr_ex_2 = []\n",
    "arr_ex_3 = []\n",
    "arr_1_ex_4 = []\n",
    "arr_2_ex_4 = []\n",
    "y_ex_4 = []\n",
    "x_ex_4 = []\n",
    "b = []\n",
    "a_1=0\n",
    "b_1=0\n",
    "a_2=0\n",
    "b_2=0\n",
    "a_3 = 0.05\n",
    "b_3 = 0.05\n",
    "b_3_temp = 10000\n",
    "a_4 = 0.05\n",
    "b_4 = 0.05\n",
    "b_4_temp = 10000\n",
    "#gauss alghorithm for 2 part\n",
    "while 0 == 0:\n",
    "    ex_4_gauss_arr_1 = []\n",
    "    ex_4_gauss_arr_2 = []\n",
    "    for a in range(0, 1000):\n",
    "        a = a / 1000\n",
    "        s = 0\n",
    "        for k in range(0, 101):\n",
    "            s += (a * x_ex_4[k] + b_3 - y_ex_4[k])**2\n",
    "        ex_4_gauss_arr_1.append(s)\n",
    "    a_3_temp = ex_4_gauss_arr_1.index(min(ex_4_gauss_arr_1)) / 1000\n",
    "    if abs(a_3 - a_3_temp) < 0.001 and abs(b_3 - b_3_temp) < 0.001:\n",
    "        break\n",
    "    a_3 = a_3_temp\n",
    "    for b in range(0, 1000):\n",
    "        b = b / 1000\n",
    "        s = 0\n",
    "        for k in range(0, 101):\n",
    "            s += (a_3 * x_ex_4[k] + b - y_ex_4[k])**2\n",
    "        ex_4_gauss_arr_2.append(s)\n",
    "    b_3_temp = ex_4_gauss_arr_2.index(min(ex_4_gauss_arr_2)) / 1000\n",
    "    if abs(b_3 - b_3_temp) < 0.001 and abs(a_3 - a_3_temp) < 0.001:\n",
    "        break\n",
    "    b_3 = b_3_temp\n",
    "while 0 == 0:\n",
    "    ex_4_gauss_arr_1 = []\n",
    "    ex_4_gauss_arr_2 = []\n",
    "    for a in range(0, 1000):\n",
    "        a = a / 1000\n",
    "        s = 0\n",
    "        for k in range(0, 101):\n",
    "            s += (a / (1 + b_4 * x_ex_4[k]) - y_ex_4[k])**2\n",
    "        ex_4_gauss_arr_1.append(s)\n",
    "    a_4_temp = ex_4_gauss_arr_1.index(min(ex_4_gauss_arr_1)) / 1000\n",
    "    if abs(a_4 - a_4_temp) < 0.001 and abs(b_4 - b_4_temp) < 0.001:\n",
    "        break\n",
    "    a_4 = a_4_temp\n",
    "    for b in range(0, 1000):\n",
    "        b = - b / 1000\n",
    "        s = 0\n",
    "        for k in range(0, 101):\n",
    "            s += (a_4 / (1 + b * x_ex_4[k]) - y_ex_4[k])**2\n",
    "        ex_4_gauss_arr_2.append(s)\n",
    "    b_4_temp = ex_4_gauss_arr_2.index(min(ex_4_gauss_arr_2)) / -1000\n",
    "    if abs(b_4 - b_4_temp) < 0.001 and abs(a_4 - a_4_temp) < 0.001:\n",
    "        break\n",
    "    b_4 = b_4_temp\n",
    "\n",
    "#brutforce algorithm for 2 part\n",
    "for a in range(0, 1000):\n",
    "    a = a / 1000\n",
    "    for b in range(0, 1000):\n",
    "        b1 = b / 1000\n",
    "        b2 = - b / 1000\n",
    "        s1 = 0\n",
    "        s2 = 0\n",
    "        for k in range(0, 101):\n",
    "            s1 += (a * x_ex_4[k] + b1 - y_ex_4[k]) ** 2\n",
    "            s2 += (a / (1 + b2 * x_ex_4[k]) - y_ex_4[k]) ** 2\n",
    "        arr_1_ex_4.append([s1, a, b1])\n",
    "        arr_2_ex_4.append([s2, a, b2])\n",
    "minimum_1=min(x1[0] for x1 in arr_1_ex_4)\n",
    "minimum_2=min(x2[0] for x2 in arr_2_ex_4)\n",
    "for sublist in arr_1_ex_4:\n",
    "    if sublist[0] == minimum_1:\n",
    "        a_1 = sublist[1]\n",
    "        b_1 = sublist[2]\n",
    "        break\n",
    "for sublist in arr_2_ex_4:\n",
    "    if sublist[0] == minimum_2:\n",
    "        a_2 = sublist[1]\n",
    "        b_2 = sublist[2]\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42945817134121878c7cd0aa3e8441d4811e76c78f56d3a59be176ac2ac0f259"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
